{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1876d245",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.8.0\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import os\n",
    "import glob\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "from numpy import unique\n",
    "from numpy import where\n",
    "from matplotlib import pyplot\n",
    "from sklearn.cluster import AffinityPropagation\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "#import tensorflow_decision_forests as tfdf\n",
    "\n",
    "from sklearn.datasets import make_classification\n",
    "\n",
    "print(tf.__version__)\n",
    "\n",
    "# Make NumPy printouts easier to read.\n",
    "np.set_printoptions(precision=3, suppress=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64fe431e",
   "metadata": {},
   "source": [
    "### Club all the traces together into a single dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4adecc06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./traces_core/dpotrf_T-200_C-4-1.prof.h5\n"
     ]
    }
   ],
   "source": [
    "path = './traces_core'# use your path\n",
    "#all_files = glob.glob(path + \"/*.h5\")\n",
    "all_files = glob.glob(path + \"/dpotrf_T-200_C-4-*.prof*\")\n",
    "\n",
    "li = []\n",
    "\n",
    "for filename in all_files:\n",
    "    trace =  pd.HDFStore(filename)\n",
    "    data = trace.get('/events')\n",
    "    trace.close()\n",
    "    \n",
    "    li.append(data)\n",
    "    print(filename)\n",
    "\n",
    "df = pd.concat(li, axis=0, ignore_index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1f6924e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(li))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8f0d6ba",
   "metadata": {},
   "source": [
    "# Clean the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "702d9c40",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[['begin', 'end', 'taskpool_id', 'task_class_id','chore_id', 'nb_data_items', 'total_data_size', 'priority']].copy()\n",
    "df['exec_time'] = (df['end'] - df['begin']) * .001 #0.001 nano seconds to micro seconds\n",
    "df = df.astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd301c4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_class_name = {0: 'dpotrf', 1: 'dtrsm', 2: 'dsyrk', 3: 'dgemm'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85b83021",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "filter = ( (df['task_class_id'] >= 0 ) & ( df['task_class_id'] <= 3 ) & ( df['priority'] >= 0 ) )\n",
    "#remove all other classes other than dpotrf, dgemm, trsm and syrk\n",
    "df = df[filter]\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "929c327c",
   "metadata": {},
   "source": [
    "## integrate likelihood of the data being in cash.\n",
    "we assume that all the task whose execution time is in the first quartile could have had all its data in the cache. While, the rest of the tasks would have resulted in a cache flush. While predicting we assume that the that the data of the task is not in the cache "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4aea73e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df['task_class_id'] == 3].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53385583",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.reset_index(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c14b7b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "task_class_id = [0,1,2,3]\n",
    "\n",
    "for i in task_class_id:\n",
    "    \n",
    "    df_exaxmple = df\n",
    "    \n",
    "    filter = (df_exaxmple['task_class_id'] == i )\n",
    "    df_refined = df_exaxmple[filter]\n",
    "    \n",
    "    print('----------------- ' + df_class_name[i] + ' [before] -------------------------- ')\n",
    "    print('Min = ' + str(df_refined['exec_time'].min()))\n",
    "    print('Max = ' + str(df_refined['exec_time'].max()))\n",
    "    print('Avg = ' + str(sum(df_refined['exec_time']) / len(df_refined['exec_time'])))\n",
    "    print('Std = ' + str(df_refined['exec_time'].std()))\n",
    "    datapoint1 = df_refined.shape[0]\n",
    "    \n",
    "    Q1 = df_refined['exec_time'].quantile(0.25)\n",
    "    Q3 = df_refined['exec_time'].quantile(0.75)\n",
    "    IQR = Q3 - Q1    #IQR is interquartile range.\n",
    "    filter = (df_refined['exec_time'] >= Q1 - 1.5 * IQR) & (df_refined['exec_time'] <= Q3 + 1.5 *IQR)\n",
    "    df_refined = df_refined.loc[filter]\n",
    "    df_refined = df_refined.astype(float)\n",
    "    \n",
    "    print('----------------- ' + df_class_name[i] + ' [after] -------------------------- ')\n",
    "    print('Min = ' + str(df_refined['exec_time'].min()))\n",
    "    print('Max = ' + str(df_refined['exec_time'].max()))\n",
    "    print('Avg = ' + str(sum(df_refined['exec_time']) / len(df_refined['exec_time'])))\n",
    "    print('Std = ' + str(df_refined['exec_time'].std()))\n",
    "    datapoint2 = df_refined.shape[0]\n",
    "    \n",
    "    perc_of_data = (datapoint1 - datapoint2) / datapoint1 * 100\n",
    "    print('datapoints removed ' + str(perc_of_data) + '%')\n",
    "    \n",
    "    ax = df_refined.plot(y='exec_time',  use_index=True,\n",
    "                    ylabel='time(mi sec) ', xlabel='index',\n",
    "                    marker='.', linestyle='none',\n",
    "                    title=df_class_name[i])\n",
    "    ax.axhline(sum(df_refined['exec_time']) / len(df_refined['exec_time']), c='r')\n",
    "    plt.savefig(df_class_name[i]+'_cloud', dpi=300)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd5f5afd",
   "metadata": {},
   "source": [
    "## K-means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d14ac88",
   "metadata": {},
   "outputs": [],
   "source": [
    "task_class_id = [0,1,2,3]\n",
    "\n",
    "for i in task_class_id:\n",
    "    # define dataset\n",
    "    df_exaxmple = df\n",
    "    filter = (df_exaxmple['task_class_id'] == i )\n",
    "    df_refined = df_exaxmple[filter]\n",
    "    x = df_refined[['index', 'exec_time']].copy()\n",
    "    X = x.to_numpy()\n",
    "    # define the model\n",
    "    model = KMeans(n_clusters=3)\n",
    "    # fit the model\n",
    "    model.fit(X)\n",
    "    # assign a cluster to each example\n",
    "    yhat = model.predict(X)\n",
    "    # retrieve unique clusters\n",
    "    clusters = unique(yhat)\n",
    "    # create scatter plot for samples from each cluster\n",
    "    for cluster in clusters:\n",
    "        # get row indexes for samples with this cluster\n",
    "        row_ix = where(yhat == cluster)\n",
    "        # create scatter of these samples\n",
    "        pyplot.scatter(X[row_ix, 0], X[row_ix, 1])\n",
    "        pyplot.title(df_class_name[i])\n",
    "    # show the plot\n",
    "    pyplot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb872227",
   "metadata": {},
   "source": [
    "## Reorder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a525cce0",
   "metadata": {},
   "outputs": [],
   "source": [
    "task_class_id = [0,1,2,3]\n",
    "\n",
    "for i in task_class_id:\n",
    "    \n",
    "    df_exaxmple = df\n",
    "    \n",
    "    filter = (df_exaxmple['task_class_id'] == i )\n",
    "    df_refined = df_exaxmple[filter]\n",
    "    df_refined = df_refined.sort_values('exec_time', axis=0, ascending=True, \n",
    "                                        inplace=False, kind='quicksort', na_position='last')\n",
    "    df_refined.insert(0, 'new_index', range(0, len(df_refined)))\n",
    "    \n",
    "    #print(df_refined.head())\n",
    "    \n",
    "    \n",
    "    Q1 = df_refined['exec_time'].quantile(0.25)\n",
    "    Q3 = df_refined['exec_time'].quantile(0.75)\n",
    "    IQR = Q3 - Q1    #IQR is interquartile range.\n",
    "    filter = (df_refined['exec_time'] >= Q1 - 1.5 * IQR) & (df_refined['exec_time'] <= Q3 + 1.5 *IQR)\n",
    "    df_refined = df_refined.loc[filter]\n",
    "    df_refined = df_refined.astype(float)\n",
    "    \n",
    "    \n",
    "    \n",
    "    ax = df_refined.plot(y='exec_time',  x='new_index',\n",
    "                    ylabel='time(mi sec) ', xlabel='index',\n",
    "                    marker='.', linestyle='none',\n",
    "                    title=df_class_name[i])\n",
    "    ax.axhline(sum(df_refined['exec_time']) / len(df_refined['exec_time']), c='r')\n",
    "    plt.savefig(df_class_name[i]+'_reorder', dpi=300)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9709c32c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cae90b8f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
